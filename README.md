# LLM Council

Chat with multiple LLMs simultaneously and compare their responses side-by-side.

## Features

- ğŸ¤– **Multi-model chat** - Send one message, get responses from all selected LLMs
- ğŸ“Š **Side-by-side comparison** - View all model responses in a grid layout
- ğŸ’¬ **Streaming responses** - Real-time token display via WebSocket
- ğŸ“š **Conversation history** - Auto-saved to database, easily reload past chats
- ğŸ¨ **Modern UI** - Clean, responsive design with TailwindCSS
- ğŸ“ˆ **Usage tracking** - Monitor token usage and estimated costs per model
- ğŸ”Œ **Dynamic model discovery** - Automatically detects available models from LiteLLM

## Supported Providers

Models are dynamically discovered from LiteLLM. Just add your API key and all available models from that provider will appear:

| Provider | API Key Variable | Example Models |
|----------|------------------|----------------|
| **OpenAI** | `OPENAI_API_KEY` | GPT-4o, GPT-4o Mini, o1, o3-mini |
| **Anthropic** | `ANTHROPIC_API_KEY` | Claude 4 Sonnet, Claude 3.5 Sonnet/Haiku, Claude 3 Opus |
| **Google Gemini** | `GEMINI_API_KEY` | Gemini 2.0 Flash, Gemini 1.5 Pro/Flash |
| **xAI** | `XAI_API_KEY` | Grok 2, Grok 3 |
| **Mistral** | `MISTRAL_API_KEY` | Mistral Large, Mixtral |
| **Cohere** | `COHERE_API_KEY` | Command R, Command R+ |
| **Together AI** | `TOGETHER_API_KEY` | Llama, Mixtral (open source) |
| **Groq** | `GROQ_API_KEY` | Llama 3.3, Mixtral (fast inference) |
| **DeepSeek** | `DEEPSEEK_API_KEY` | DeepSeek Chat, DeepSeek Reasoner |
| **Perplexity** | `PERPLEXITY_API_KEY` | Sonar, Sonar Pro (with search) |

## Quick Start

### Prerequisites

- [Docker Desktop](https://www.docker.com/products/docker-desktop/) (for Mac/Windows)
- API keys from at least one LLM provider

### 1. Clone and Setup

```bash
cd llm_council

# Create environment file
cp .env.example .env
```

### 2. Add Your API Keys

Edit `.env` and add your API keys:

```bash
# Add any combination of these - models auto-discover based on available keys
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxx
ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxxxxxxxxx
GEMINI_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxx
XAI_API_KEY=xai-xxxxxxxxxxxxxxxxxxxx

# Optional: Additional providers
MISTRAL_API_KEY=xxxxxxxxxxxxxxxxxxxx
COHERE_API_KEY=xxxxxxxxxxxxxxxxxxxx
TOGETHER_API_KEY=xxxxxxxxxxxxxxxxxxxx
GROQ_API_KEY=xxxxxxxxxxxxxxxxxxxx
DEEPSEEK_API_KEY=xxxxxxxxxxxxxxxxxxxx
PERPLEXITY_API_KEY=xxxxxxxxxxxxxxxxxxxx
```

### 3. Run with Docker Compose

```bash
# Start the app
docker-compose up --build

# Or for development with hot reload:
docker-compose -f docker-compose.dev.yml up --build
```

### 4. Open the App

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

## Running Without Docker

### Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt

# Run server
uvicorn app.main:app --reload --port 8000
```

### Frontend

```bash
cd frontend

# Install dependencies
npm install

# Run dev server
npm run dev
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (React + Vite)                      â”‚
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   History    â”‚    â”‚              Response Grid               â”‚   â”‚
â”‚   â”‚   Sidebar    â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚   â”‚              â”‚    â”‚   â”‚  GPT-4o  â”‚ â”‚  Claude  â”‚ â”‚ Gemini  â”‚ â”‚   â”‚
â”‚   â”‚   + Models   â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚
â”‚                       â”‚   â”‚          Chat Input                 â”‚â”‚   â”‚
â”‚                       â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ WebSocket
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Backend (FastAPI + LiteLLM)                      â”‚
â”‚                                                                      â”‚
â”‚   â€¢ WebSocket /ws/chat  â†’ Streams responses from all models         â”‚
â”‚   â€¢ GET /history        â†’ List conversations                        â”‚
â”‚   â€¢ GET /history/{id}   â†’ Get conversation                          â”‚
â”‚   â€¢ Auto-saves to SQLite                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  OpenAI   â”‚ Anthropic â”‚  Google   â”‚    xAI    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
llm_council/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI entry point
â”‚   â”‚   â”œâ”€â”€ config.py         # Settings & API keys
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py   # SQLite connection
â”‚   â”‚   â”‚   â””â”€â”€ models.py     # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # WebSocket endpoint
â”‚   â”‚   â”‚   â””â”€â”€ history.py    # REST API for history
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ llm_service.py    # LiteLLM wrapper
â”‚   â”‚       â””â”€â”€ history_service.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInput.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ HistorySidebar.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelSelector.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ResponseGrid.tsx
â”‚   â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â”‚   â””â”€â”€ chatStore.ts  # Zustand state
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ api.ts
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.dev.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## API Reference

### WebSocket: `/chat/ws`

Send a chat message:
```json
{
  "type": "chat",
  "conversation_id": "uuid-or-null",
  "message": "Hello!",
  "models": ["gpt-4o", "claude-3-5-sonnet-20240620"]
}
```

Receive streamed responses:
```json
{"type": "conversation_started", "conversation_id": "uuid"}
{"type": "token", "model_id": "gpt-4o", "token": "Hello"}
{"type": "model_complete", "model_id": "gpt-4o", "content": "...", "latency_ms": 1234}
{"type": "chat_complete", "conversation_id": "uuid"}
```

### REST API

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/chat/models` | GET | List available models |
| `/history` | GET | List conversations |
| `/history/{id}` | GET | Get conversation |
| `/history/{id}` | DELETE | Delete conversation |

## Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `OPENAI_API_KEY` | OpenAI API key | At least one |
| `ANTHROPIC_API_KEY` | Anthropic API key | API key |
| `GEMINI_API_KEY` | Google Gemini API key | required |
| `XAI_API_KEY` | xAI (Grok) API key | |
| `DATABASE_URL` | SQLite connection string | No (default provided) |
| `DEBUG` | Enable debug mode | No (default: true) |

### Getting API Keys

| Provider | URL |
|----------|-----|
| OpenAI | https://platform.openai.com/api-keys |
| Anthropic | https://console.anthropic.com/ |
| Google Gemini | https://aistudio.google.com/apikey |
| xAI | https://console.x.ai/ |

## Development

### Running Tests

```bash
cd backend
pytest
```

### Building for Production

```bash
docker-compose -f docker-compose.yml build
```

## License

MIT
